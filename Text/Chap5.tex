\chapter{Numerical experiments}

In this chapter we are going to measure the performance of our algorithms by applying them to an example of the weak problem \eqref{weakProb}. For this purpose, the solution of this problem is derived first. Afterwards, we compare the analytical solution with the solution from the FOM-EnOpt and the AML-EnOpt algorithm. Additionally, we examine how the optimization algorithms behave if parameters such as the step size are changed or if other neural network training parameters are used.

\section{Example of an analytical problem}

Here we consider the problem \eqref{weakProb} on $\Omega\times I=(0,1)^2\times(0,0.1)$ with homogeneous Dirichlet boundary conditions. The following example is originally described in \cite{doi:10.1137/070694016}.

To define the functions in \eqref{weakProb}, we use the eigenfunction
\begin{displaymath}
w_a(t,x_1,x_2):=\operatorname{exp}(a\pi^2t)\operatorname{sin}(\pi x_1)\operatorname{sin}(\pi x_2) \text{ for } a\in\mathbb{R}.
\end{displaymath}

Now we set

\begin{eqnarray*}
f(t,x_1,x_2)&:=&-\pi^4w_a(T,x_1,x_2),\\
\hat{u}(t,x_1,x_2)&:=&\frac{a^2-5}{2+a}\pi^2w_a(t,x_1,x_2)+2\pi^2w_a(T,x_1,x_2),\\
u_0(x_1,x_2)&:=&\frac{-1}{2+a}\pi^2w_a(0,x_1,x_2).
\end{eqnarray*}

If we set the regularization parameter $\alpha$ in the objective functional \eqref{objFun} as $\pi^{-4}$, we get the optimal solution $(\bar{q}, \bar{u})$, where
%$(\bar{q}, \bar{u}, \bar{z})$, where

\begin{eqnarray*}
\bar{q}(t,x_1,x_2)&:=&-\pi^4\left(w_a(t,x_1,x_2)-w_a(T,x_1,x_2)\right),\\
\bar{u}(t,x_1,x_2)&:=&\frac{-1}{2+a}\pi^2w_a(t,x_1,x_2).%,\\
%\bar{z}(t,x_1,x_2)&:=&w_a(t,x_1,x_2)-w_a(T,x_1,x_2).
\end{eqnarray*}


\section{Numerical results}

%\footnotesize
\begin{tabular}{ll}
\hline
Parameter & Value\\
\hline
Elements of the initial constant control vector $\mathbf{q}_0$ & -40\\
Initial step size $\beta_1$ & $1$\\
Initial covariance matrix adaption step size $\beta_2$ & $1$\\
Initial trust region step size $\delta_\mathrm{init}$ & $50$\\
Step size contraction $r$ & $0.5$\\
Maximum step size trials $\nu^*$ & $10$\\
Initial control-type variance $\sigma^2_1$ & $0.1$\\
Constant correlation factor $\rho$ & $0.9$\\
Perturbation size $N$ & $100$\\
FOM-EnOpt $\varepsilon$ & $10^{-8}$\\
Tolerances Adaptive-ML-EnOpt inner iteration $\varepsilon_i$ & $10^{-8}$\\
Adaptive-ML-EnOpt outer iteration $\varepsilon_o$ & $10^{-8}$\\
%& $$\\
\hline
\end{tabular}

%\footnotesize
\begin{tabular}{|l|l|l|l|}
\hline
Method & Iteration & FOM value  & Surrogate value \\%& FOM eval. & Surrogate eval. & Training time (min) & $T_\mathrm{total}$ (min) & Speedup\\
\hline
\hline
$\mathrm{FOM-EnOpt}$ & $1$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $2$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
 \hline
$\mathrm{AML-EnOpt}$ & $1$ & $$ & $$ \\%& $407$ & $12315$ & $2.03$ & $8.87$ & $6.18$ \\
\cline{2-4}
 & $2$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\hline
\multicolumn{4}{l}{}\\
\hline
Method & Iteration & Outer iterations  & Inner iterations \\%& FOM eval. & Surrogate eval. & Training time (min) & $T_\mathrm{total}$ (min) & Speedup\\
\hline
\hline
$\mathrm{FOM-EnOpt}$ & $1$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $2$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
 \hline
$\mathrm{AML-EnOpt}$ & $1$ & $$ & $$ \\%& $407$ & $12315$ & $2.03$ & $8.87$ & $6.18$ \\
\cline{2-4}
 & $2$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\hline
\multicolumn{4}{l}{}\\
\hline
Method & Iteration & FOM evaluations  & Surrogate evaluations \\%& FOM eval. & Surrogate eval. & Training time (min) & $T_\mathrm{total}$ (min) & Speedup\\
\hline
\hline
$\mathrm{FOM-EnOpt}$ & $1$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $2$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
 \hline
$\mathrm{AML-EnOpt}$ & $1$ & $$ & $$ \\%& $407$ & $12315$ & $2.03$ & $8.87$ & $6.18$ \\
\cline{2-4}
 & $2$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\hline
\multicolumn{4}{l}{}\\
\hline
Method & Iteration &Training time (min)  & Total run time (min) \\%& FOM eval. & Surrogate eval. & Training time (min) & $T_\mathrm{total}$ (min) & Speedup\\
\hline
\hline
$\mathrm{FOM-EnOpt}$ & $1$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $2$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & - \\%& $2839$ & - & - & $54.86$ & - \\
 \hline
$\mathrm{AML-EnOpt}$ & $1$ & $$ & $$ \\%& $407$ & $12315$ & $2.03$ & $8.87$ & $6.18$ \\
\cline{2-4}
 & $2$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\cline{2-4}
 & $3$ & $$ & $$ \\%& $2839$ & - & - & $54.86$ & - \\
\hline
\end{tabular}