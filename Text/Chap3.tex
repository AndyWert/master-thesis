\chapter{ Ensembleâ€‘based optimization algorithm}

The adaptive ensemble-based algorithm (EnOpt) is usually used to maximize the net present value of oil recovery methods with respect to a control vector. Examples are presented in \cite{Keil2022-dj, OGUNTOLA2021109165, Zhang2023-sg}. In this chapter, we want to utilize the EnOpt algorithm to optimize the objective function $j$. Our implementation is similar to that in \cite{Keil2022-dj}.

We begin by describing this algorithm for a general function $F:\mathbb{R}^{N_q}\to \mathbb{R}$ to iteratively solve the optimization problem
\begin{displaymath}
\operatorname*{maximize}_{\mathbf{q}\in\mathcal{D}}F(\mathbf{q}).
\end{displaymath}

We start at an initialization $\mathbf{q}_0$,  which is updated iteratively with a preconditioned gradient ascent method that is given by
\begin{displaymath}
\mathbf{q}_{k+1} = \mathbf{q}_k+\beta_k\mathbf{d}_k,
\end{displaymath}
\begin{displaymath}
\mathbf{d}_k\approx\frac{\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k}{\|\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k\|}_\infty,
\end{displaymath}
where $k=0,1,2,\dotsc$ denotes the optimization iteration. $\beta_k$ with $\beta_k>0$ is computed by using a line search. Furthermore, $\mathbf{C}_{\mathbf{q}_k}^k$ denotes the user-defined covariance matrix of the control variables at the $k$-th iteration and $\mathbf{G}_k$ is the approximate gradient of $F$ with respect to the control variables.

We define the initial covariance matrix $\mathbf{C}_{\mathbf{q}_0}^0$ so that the covariance between controls of different basis functions $\phi_i,\phi_j$ is zero and
\begin{displaymath}
\mathrm{Cov}(q_j^i,q_j^{i+h})=\sigma_j^2\rho^h\left(\frac{1}{1-\rho^2}\right),\text{ for all }h\in\{0,\dotsc,M-i\},
\end{displaymath}
where $\sigma_j^2>0$ is the variance for the basis function $\phi_j$ and $\rho\in(-1,1)$ the correlation coefficient.

That means that for $\mathbf{C}_j:=\left(\mathrm{Cov}(q_j^i,q_j^{k})\right)_{i,k}$ with $j=1,\dotsc,N_b$, we set
\begin{equation}
\label{initCov}
\mathbf{C}_{\mathbf{q}_0}^0 =
\begin{pmatrix}
  \mathbf{C}_1 & 0 & \cdots & 0 \\
  0 & \mathbf{C}_2 & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \mathbf{C}_{N_b} 
 \end{pmatrix}.
\end{equation}

To compute the step direction $\mathbf{d}_k$ at iteration step $k$, we sample $\mathbf{q}_{k,m}\in\mathcal{D}$ for $m=1,\dotsc,N$, with $N\in\mathbb{N}$, from a multivariate Gaussian distribution with mean $\mathbf{q}_k$ and covariance $\mathbf{C}_{\mathbf{q}_k}^k$, and then we define
\begin{equation}
\label{CqkF}
\mathbf{C}_{\mathbf{q}_k,F}^k:=\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k)).
\end{equation}
Now, we set $\mathbf{d}_k=\frac{\mathbf{C}_{\mathbf{q}_k,F}^k}{\|\mathbf{C}_{\mathbf{q}_k,F}^k\|_\infty}$. This is valid since $\mathbf{C}_{\mathbf{q}_k,F}^k$ is an estimation of $\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k$, which can by shown like in \cite{OGUNTOLA2021109165}. Here, we begin with the Taylor expansion around $\mathbf{q}_k$ and get
\begin{eqnarray*}
&&F(\mathbf{q})=F(\mathbf{q}_k)+(\mathbf{q}-\mathbf{q}_k)^T\nabla F(\mathbf{q}_k)+O(\|\mathbf{q}-\mathbf{q}_k\|^2)\\
&\implies&F(\mathbf{q})-F(\mathbf{q}_k)=(\mathbf{q}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}-\mathbf{q}_k\|^2).
\end{eqnarray*}
Multiplying both sides by $(\mathbf{q}-\mathbf{q}_k)$ and setting $\mathbf{q}=\mathbf{q}_{k,m}$ yields
\begin{eqnarray*}
&&(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
&=&(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3),
\end{eqnarray*}
where $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ are the remaining terms containing order $\geq3$ of $(\mathbf{q}_{k,m}-\mathbf{q}_k)$. Neglecting $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ gives by summation over all samples and multiplication of both sides with $\frac{1}{N-1}$:
\begin{eqnarray*}
&&\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
&\approx&\left(\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\right)\mathbf{G}_k\\
&\implies&\mathbf{C}_{\mathbf{q}_k,F}^k\approx\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k,
\end{eqnarray*}
since $\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T$ is itself an approximation of $\mathbf{C}_{\mathbf{q}_k}^k$.\\

By using the samples $\{\mathbf{q}_{k-1,m}\}_{m=1}^N$ and the covariance matrix $\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}$ from the last iteration, we update $\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}$, like in \cite{Stordal2016-cj}, by setting
\begin{displaymath}
\mathbf{C}_{\mathbf{q}_{k}}^{k}=\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}+\tilde{\beta}_k\tilde{\mathbf{d}}_k\text{ with}
\end{displaymath}
\begin{displaymath}
\tilde{\mathbf{d}}_k=N^{-1}\sum_{m=1}^N(F(\mathbf{q}_{k-1,m})-F(\mathbf{q}_k))((\mathbf{q}_{k-1,m}-\mathbf{q}_k)(\mathbf{q}_{k-1,m}-\mathbf{q}_k)^T-\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}),
\end{displaymath}
where $\tilde{\beta}_k$ is a step size that is chosen so that no entries of the diagonal of $\mathbf{C}_{\mathbf{q}_{k}}^{k}$ are negative. How we set $\tilde{\beta}_k$ is shown below at the implementation of the entire EnOpt algorithm.

Now that we have described the optimization steps of this algorithm, we iterate until $F(\mathbf{q}_k)\leq F(\mathbf{q}_{k-1})+\varepsilon$, where $\varepsilon>0$. In our implementation, the EnOpt algorithm takes the objective function $F:\mathbb{R}^{N_q}\to\mathbb{R}$, our initial iterate $\mathbf{q}_0\in\mathbb{R}^{N_q}$, the sample size $N\in\mathbb{N}$, the tolerance $\varepsilon>0$, the maximum number of iterations $k^*\in\mathbb{N}$, the initial step size $\beta>0$ for the computation of the next iterate, the initial step size $\tilde{\beta}$ for the iteration of the covariance matrix, the step size contraction $r\in(0,1)$, the maximum number of step size trials $\nu^*\in\mathbb{N}$, the variance $\sigma^2\in\mathbb{R}^{N_b}$ with positive elements, the correlation coefficient $\rho\in(-1,1)$ and a projection $\operatorname{pr}$, where the default is the identity function $\operatorname{id}:x\to x$.

The implementation in pseudo code is shown here:
\begin{algorithm}[H]%\footnotesize
\caption{EnOpt algorithm}
\begin{algorithmic}[1]
\Procedure{EnOpt}{$F,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho,\operatorname{pr}=\operatorname{id}$}
\State $F_{{0}}\gets F(\mathbf{q}_0)$
\State $\mathbf{q}_{1},T_{1},\mathbf{C}_{\mathbf{q}_{0}}^{0},F_{1}\gets\mathrm{OptStep}(F,\mathbf{q}_0,N,0,\{\},0,F_{0},\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
\State $k\gets 1$
\While{$F_{k}>F_{k-1}+\varepsilon$\text{ and }$k<k^*$}
\State $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_{\mathbf{q}_{k}}^{k},F_{k+1}\gets\mathrm{OptStep}(F,\mathbf{q}_k,N,k,T_k,\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
\State $k\gets k+1$
\EndWhile
\State \Return $\mathbf{q}^*\gets\mathbf{q}_k$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{OptStep algorithm}
\begin{algorithmic}[1]
\Procedure{OptStep}{$F,\mathbf{q}_k,N,k,T_k,\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr}$}
\If{$k=0$}
\State \text{Compute the initial covariance matrix }$\mathbf{C}_{\mathbf{q}_0}^0$\text{ like defined in }\eqref{initCov}\text{ with }$\mathbf{q}_0,\sigma^2,\rho$
\Else
\State $\mathbf{C}_{\mathbf{q}_{k}}^{k} \gets \mathrm{updateCov}(\mathbf{q}_k, N, T_k, \mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}, F_k ,\tilde{\beta})$
\EndIf
\State \text{Sample }$N$\text{ control vectors }$\{\mathbf{q}_{k,j}\}_{j=1}^N$\text{ from a distribution }$\mathcal{N}(\mathbf{q}_{k},\mathbf{C}_{\mathbf{q}_{k}}^{k})$
\State $T_{k+1}\gets\{\mathbf{q}_{k,j},F(\mathbf{q}_{k,j})\}_{j=1}^N$
\State \text{Compute the vector }$\mathbf{C}_{\mathbf{q}_{k},F}^{k}$\text{ with }$\mathbf{q}_k,F_k$\text{ and the stored values of }$T_{k+1}\text{ like in }\eqref{CqkF}$
\State \text{Compute the search direction }$\mathbf{d}_k=\mathbf{C}_{\mathbf{q}_{k},F}^{k}/\|\mathbf{C}_{\mathbf{q}_{k},F}^{k}\|_\infty$
\State $\mathbf{q}_{k+1}\gets\mathrm{LineSearch}(F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr})$
\State $F_{k+1}\gets F(\mathbf{q}_{k+1})$
\State \Return $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_{\mathbf{q}_k}^k,F_{k+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
The next algorithm uses some functions from the Python package NumPy, which is imported here as $\mathrm{np}$.
\begin{algorithm}[H]%\footnotesize
\caption{Covariance matrix update}
\begin{algorithmic}[1]
\Procedure{updateCov}{$\mathbf{q}_k, N, T_k, \mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}, F_k ,\tilde{\beta}$}
\State $N_q\gets \mathrm{len}(\mathbf{q}_k)$
\State $\mathbf{d}_k \gets \mathrm{np.zeros}((N_q, N_q))$
\State $\mathrm{assert}$ $\mathrm{len}(T_k) == N$
\For{$m=0,\dotsc,N-1$}
\State $\mathbf{d}_k \gets d_k + (T_k[m][1]-F_k)*((T_k[m][0]-\mathbf{q}_k).\mathrm{reshape}((N_q,1))*(T_k[m][0]-\mathbf{q}_k).\mathrm{reshape}((1,N_q))-\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1})$
\EndFor
\State $\mathbf{d}_k \gets \mathbf{d}_k/N$
\State $\mathbf{C}_\mathrm{diag}\gets\mathrm{np.zeros}(N_q)$
\State $\mathbf{d}_\mathrm{diag}\gets\mathrm{np.zeros}(N_q)$
\For{$i=0,\dotsc,N_q-1$}
\State $\mathbf{C}_\mathrm{diag}[i]\gets\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}[i, i]$
\State $\mathbf{d}_\mathrm{diag}[i]\gets\mathbf{d}_k[i, i]$
\EndFor
\State $\tilde{\beta}_k\gets\tilde{\beta}$
\While{$\mathrm{np.min}(\mathbf{C}_\mathrm{diag}+\tilde{\beta}_k*\mathbf{d}_k)<0$}
\State $\tilde{\beta}_k \gets \tilde{\beta}_k/2$
\EndWhile
\State \Return $\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1}+\tilde{\beta}_k*\mathbf{d}_k$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{Line search}
\begin{algorithmic}[1]
\Procedure{LineSearch}{$F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr}$}
\State $\beta_k \gets \beta$
\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
\State $\nu \gets 0$
\While{$F(\mathbf{q}_{k+1})-F(\mathbf{q}_k)\leq\varepsilon$\text{ and }$\nu<\nu^*$}
\State $\beta_k \gets r\beta_k$
\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
\State $\nu \gets \nu+1$
\EndWhile
\Return $\mathbf{q}_{k+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
Now we use this algorithm to optimize our objective function $j$. Since this is a maximization procedure and $j$ should be minimized, we apply $-j$ to the EnOpt algorithm, which gives us:
\begin{algorithm}[H]%\footnotesize
\caption{FOM-EnOpt algorithm}
\begin{algorithmic}[1]
\Procedure{FOM-EnOpt}{$\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho$}
\State \Return $\mathrm{EnOpt}(-j,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho)$
\EndProcedure
\end{algorithmic}
\end{algorithm}