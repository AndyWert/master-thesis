\chapter{\label{ChapterEnsembleBasedOptimizationAlgorithm}Ensembleâ€‘based optimization algorithm}

The adaptive ensemble-based optimization (EnOpt) algorithm is often used to maximize the net present value of oil recovery methods with respect to a control vector. Examples are presented in \cite{Keil2022-dj, OGUNTOLA2021109165, Zhang2023-sg}. In this chapter, we want to utilize the EnOpt algorithm to optimize the objective functional $j$. Our implementation is similar to that in \cite{Keil2022-dj}.

In the following, the EnOpt algorithm is presented for a general functional $F:\mathbb{R}^{N_\mathbf{q}}\to \mathbb{R}$ to iteratively solve the optimization problem
\begin{displaymath}
\operatorname*{maximize}_{\mathbf{q}\in\mathcal{D}}F(\mathbf{q}).
\end{displaymath}

We start at an initialization $\mathbf{q}_0$,  which is updated iteratively with a preconditioned gradient ascent method that is given by
\begin{displaymath}
\mathbf{q}_{k+1} = \mathbf{q}_k+\beta_k\mathbf{d}_k,
\end{displaymath}
\begin{displaymath}
\mathbf{d}_k\approx\frac{\mathbf{C}_k\mathbf{G}_k}{\|\mathbf{C}_k\mathbf{G}_k\|_\infty},
\end{displaymath}
where $k=0,1,2,\dotsc$ denotes the iteration of the optimization. The step size $\beta_k$, with $0<\beta_k\leq1$, is computed by using a line search procedure. We mentioned in the introduction that we use a sample around the iterate $\mathbf{q}_k$ to calculate the next iterate $\mathbf{q}_{k+1}$. The matrix $\mathbf{C}_k\in\mathbb{R}^{N_\mathbf{q}\times N_\mathbf{q}}$ denotes the covariance matrix of this sample set at the $k$-th iteration. Let $\mathbf{E}_k$ be the expectation with respect to to a multivariate Gaussian distribution with the mean $\mathbf{q}_k$ and the covariance $\mathbf{C}_k$. The vector $\mathbf{G}_k\in\mathbb{R}^{N_\mathbf{q}}$ is here defined as $\mathbf{G}_k=\nabla_{\mathbf{q}_k}\mathbf{E}_k[F(X)]$, where $\nabla_{\mathbf{q}_k}$ denotes the gradient with respect to the mean $\mathbf{q}_k$ of the Gaussian distribution. So in this definition, the step direction $\mathbf{d}_k$ is approximately $\mathbf{G}_k$, preconditioned with the covariance matrix $\mathbf{C}_k$ and normalized by dividing by the maximum norm of itself. Since we do not have access to the gradient of $\mathbf{E}_k[F(X)]$ with respect to $\mathbf{q}_k$, we use an estimator of $\mathbf{C}_k\mathbf{G}_k/\|\mathbf{C}_k\mathbf{G}_k\|_\infty$ as the step direction. A more detailed description of the EnOpt algorithm is shown next.\\

We define the initial covariance matrix $\mathbf{C}_0$ so that the covariance between controls of different shape functionals $\phi_i,\phi_j\in\Phi$ from \eqref{basisFuncionsList} is zero. That means, we have for $i,j=1,\dotsc,N_b$ with $i\neq j$:
\begin{equation*}
\mathrm{Cov}(q_i^m,q_j^n)=0,\text{ for all }m,n\in\{0,\dotsc,N_t\}.
\end{equation*}
We expect controls of the same shape functional $\phi_j\in\Phi$ to correlate. Based on that, we set the controls for $j=1,\dotsc,N_b$ and $m=0,\dotsc,N_t$ like in \cite{Keil2022-dj} as
\begin{equation}
\label{defineInitialCovariance}
\mathrm{Cov}(q_j^m,q_j^{m+h})=\sigma_j^2\rho^h\left(\frac{1}{1-\rho^2}\right),\text{ for all }h\in\{0,\dotsc,N_t-i\},
\end{equation}
where $\sigma_j^2>0$ is the variance for the basis functional $\phi_j$ and $\rho\in(-1,1)$ the correlation coefficient. This defines the covariances between all controls since $\mathrm{Cov}(q_j^m,q_j^{m+h})=\mathrm{Cov}(q_j^{m+h},q_j^m)$. By adjusting the variance and correlation coefficient, we can influence how the controls of the same shape functional depend on each other.

For example, if $\rho$ is close to 1, then the covariance of the controls is large, resulting in them to show a similar behaviour. That means, if a control variable of a shape functional at time step $m$ is large, then the control variables of this shape functional also tend to be large at time steps close to $m$. Analogously, if a control variable of a shape functional at time step $m$ is small, then the control variables of this shape functional at time steps close to $m$ have a tendency to be small, too.

If $\rho$ is close to -1, then the covariance is large when the $h$ in \eqref{defineInitialCovariance} is even and small, and the covariance is negative if $h$ is odd. This leads to high oscillations of the control variables.\\

Now that we have the covariances between the individual controls, we can set the initial covariance matrix. For that, let the matrices $\mathbf{C}^j_0\in\mathbb{R}^{(N_t+1)\times(N_t+1)}$ for $j=1,\dotsc,N_b$ be defined as $\mathbf{C}^j_0:=\left(\mathrm{Cov}(q_j^i,q_j^{k})\right)_{i,k=0,\dotsc,N_t}$. Then the initial covariance matrix is
\begin{equation}
\label{initCov}
\mathbf{C}_0 =
\begin{pmatrix}
  \mathbf{C}^1_0 & 0 & \cdots & 0 \\
  0 & \mathbf{C}^2_0 & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \mathbf{C}^{N_b}_0 
 \end{pmatrix}.
\end{equation}
\\

To compute the step direction $\mathbf{d}_k$ at iteration step $k$, we sample $\mathbf{q}_{k,m}\in\mathcal{D}$ for $m=1,\dotsc,N$, with $N\in\mathbb{N}$, from a multivariate Gaussian distribution with the mean $\mathbf{q}_k$ and the covariance $\mathbf{C}_k$. Then we define the vector $\mathbf{C}_F^k\in\mathbb{R}^{N_\mathbf{q}}$ with these samples and the current iterate $\mathbf{q}_k$ as
\begin{equation}
\label{CqkF}
\mathbf{C}_F^k:=\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k)).
\end{equation}
Now we set the step direction as $\mathbf{d}_k=\frac{\mathbf{C}_F^k}{\|\mathbf{C}_F^k\|_\infty}$. This step direction converges under certain conditions almost surely against $\frac{\mathbf{C}_k\mathbf{G}_k}{\|\mathbf{C}_k\mathbf{G}_k\|_\infty}$ for an increasing sample size. The following lemma and its corresponding proof is based on Lemma 1 in \cite{Stordal2016-cj}.
\begin{lem}
Assume $\mathbf{q}\in\mathbb{R}^n$ with $n=N_\mathbf{q}$. Let
\begin{equation*}
\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)=(2\pi)^{-n/2}\operatorname{det}(\mathbf{C}_k)^{-1/2}\operatorname{exp}\left(-\frac{1}{2}(\mathbf{q}-\mathbf{q}_k)^T\mathbf{C}^{-1}_k(\mathbf{q}-\mathbf{q}_k)\right)
\end{equation*}
denote the multivariate Gaussian density with mean $\mathbf{q}_k$ and covariance matrix $\mathbf{C}_k$. If
\begin{equation}
\label{lemmaCFKCondition}
\int_{\mathbb{R}^n}\left|(F(\mathbf{q})-F(\mathbf{q}_k))(\mathbf{q}-\mathbf{q}_k)\right|\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}<\infty,
\end{equation}
\begin{equation}
\label{lemmaCFKCondition2}
\int_{\mathbb{R}^n}|F(\mathbf{q})\mathbf{C}^{-1}_k(\mathbf{q}-\mathbf{q}_k)|\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}<\infty,
\end{equation}
and
\begin{equation}
\label{lemmaCFKCondition3}
\int_{\mathbb{R}^n}|F(\mathbf{q})|\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}<\infty,
\end{equation}
then $\mathbf{C}_F^k$ from Equation \eqref{CqkF} converges almost surely to
\begin{equation*}
\mathbf{C}_k\nabla_{\mathbf{q}_k}\int_{\mathbb{R}^n}F(\mathbf{q})\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}.
\end{equation*}
\end{lem}
\begin{proof}
Let $Y^k_m=(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))$. Then $\{Y^k_m\}_{m=1}^N$ is an i.i.d. sample from $\mathcal{N}(\mathbf{q}_k, \mathbf{C}_k)$. Also, it holds that $\mathbf{E}_k[|Y^k_m|]<\infty$ by \eqref{lemmaCFKCondition}. Now, the strong law of large numbers yields
\begin{eqnarray*}
&&\frac{1}{N-1}\sum_{m=1}^NY^k_m\\
&\xrightarrow{\mathbf{a.s.}}&\int_{\mathbb{R}^n}(F(\mathbf{q})-F(\mathbf{q}_k))(\mathbf{q}-\mathbf{q}_k)\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}\\
&=&\int_{\mathbb{R}^n}F(\mathbf{q})(\mathbf{q}-\mathbf{q}_k)\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}\\
&=&\mathbf{C}_k\int_{\mathbb{R}^n}F(\mathbf{q})\mathbf{C}^{-1}_k(\mathbf{q}-\mathbf{q}_k)\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}\\
&=&\mathbf{C}_k\int_{\mathbb{R}^n}F(\mathbf{q})\nabla_{\mathbf{q}_k}\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q},
\end{eqnarray*}
where the last equation follows from
\begin{eqnarray*}
\mathbf{C}^{-1}_k(\mathbf{q}-\mathbf{q}_k)&=&\nabla_{\mathbf{q}_k}\log(\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)),\\
\left(\nabla_{\mathbf{q}_k}\log(\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k))\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\right)&=&\nabla_{\mathbf{q}_k}\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k).
\end{eqnarray*}
\eqref{lemmaCFKCondition2} and \eqref{lemmaCFKCondition3} yield now
\begin{equation*}
\mathbf{C}_k\int_{\mathbb{R}^n}F(\mathbf{q})\nabla_{\mathbf{q}_k}\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}=\mathbf{C}_k\nabla_{\mathbf{q}_k}\int_{\mathbb{R}^n}F(\mathbf{q})\Phi(\mathbf{q}\mid \mathbf{q}_k, \mathbf{C}_k)\,\mathrm{d}\mathbf{q}
\end{equation*}
with the Leibniz integral rule, see for example Theorem 2.27 in \cite{Folland_undated-pd}.
\end{proof}



%\begin{prop}
%\label{propCovMat}
%Let $\mathbf{C}_F^k$ be defined as in \eqref{CqkF}, where the vectors $\mathbf{q}_{k,m}$ for $m=1,\dotsc,N$ are sampled from a multivariate Gaussian distribution with the mean $\mathbf{q}_k$ and the covariance $\mathbf{C}_k$. Then it holds that
%\begin{equation*}
%\mathbf{C}_F^k\approx\mathbf{C}_k\mathbf{G}_k,
%\end{equation*}
%where $\mathbf{G}_k$ is the gradient of $F$ at $\mathbf{q}_k$.
%\end{prop}
%\begin{proof}
%The proof goes according to \cite{OGUNTOLA2021109165}. We begin with the Taylor expansion around $\mathbf{q}_k$ and get for some $\mathbf{q}\in\mathcal{D}$
%\begin{eqnarray*}
%&&F(\mathbf{q})=F(\mathbf{q}_k)+(\mathbf{q}-\mathbf{q}_k)^T\nabla F(\mathbf{q}_k)+O(\|\mathbf{q}-\mathbf{q}_k\|^2)\\
%&\implies&F(\mathbf{q})-F(\mathbf{q}_k)=(\mathbf{q}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}-\mathbf{q}_k\|^2).
%\end{eqnarray*}
%Multiplying both sides by $(\mathbf{q}-\mathbf{q}_k)$ and setting $\mathbf{q}=\mathbf{q}_{k,m}$ yields
%\begin{eqnarray*}
%&&(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
%&=&(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3),
%\end{eqnarray*}
%where $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ are the remaining terms containing order $\geq3$ of $(\mathbf{q}_{k,m}-\mathbf{q}_k)$. Neglecting $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ gives by summation over all samples and multiplication of both sides with $\frac{1}{N-1}$:
%\begin{eqnarray*}
%&&\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
%&\approx&\left(\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\right)\mathbf{G}_k\\
%&\implies&\mathbf{C}_F^k\approx\mathbf{C}_k\mathbf{G}_k,
%\end{eqnarray*}
%since $\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T$ is itself an approximation of $\mathbf{C}_k$.
%\end{proof}

Using the samples $\{\mathbf{q}_{k-1,m}\}_{m=1}^N$ and the covariance matrix $\mathbf{C}_{k-1}$ from the last iteration, we update $\mathbf{C}_{k-1}$ as suggested in \cite{Stordal2016-cj} by setting
\begin{equation}
\label{updateCov}
\mathbf{C}_k=\mathbf{C}_{k-1}+\tilde{\beta}_k\tilde{\mathbf{d}}_k\text{ with}
\end{equation}
\begin{equation}
\label{updateCovDirection}
\tilde{\mathbf{d}}_k=N^{-1}\sum_{m=1}^N(F(\mathbf{q}_{k-1,m})-F(\mathbf{q}_k))((\mathbf{q}_{k-1,m}-\mathbf{q}_k)(\mathbf{q}_{k-1,m}-\mathbf{q}_k)^T-\mathbf{C}_{k-1}),
\end{equation}
where $\tilde{\beta}_k$ is a step size that is chosen so that no entries of the diagonal of $\mathbf{C}_k$ are negative. How we set $\tilde{\beta}_k$ is shown below at the implementation of the entire EnOpt algorithm. The idea behind this is to update the expected value of $F$ also with respect to the covariance matrix. We expect to get better samples in the next iteration due to the updated covariance matrix. Therefore, we call this procedure the adaptive EnOpt algorithm.

Now that we have explained the optimization steps of this algorithm, we iterate until $F(\mathbf{q}_k)\leq F(\mathbf{q}_{k-1})+\varepsilon$, where $\varepsilon>0$.\\

Next, we describe our implementation of the EnOpt algorithm. The EnOpt algorithm takes the objective function $\textproc{F}:\mathbb{R}^{N_\mathbf{q}}\to\mathbb{R}$, our initial iterate $\mathbf{q}_0\in\mathcal{D}$, the sample size $N\in\mathbb{N}$, the tolerance $\varepsilon>0$, the maximum number of iterations $k^*\in\mathbb{N}$, the initial step size $\beta_1>0$ for the computation of the next iterate, the initial step size $\beta_2>0$ for the update of the covariance matrix, the step size contraction $r\in(0,1)$, the maximum number of step size trials $\nu^*\in\mathbb{N}$, the variance $\sigma^2\in\mathbb{R}^{N_b}$ with positive elements, the correlation coefficient $\rho\in(-1,1)$, the number of time steps $N_t\in\mathbb{N}$, the number of basis functions $N_b\in\mathbb{N}$, a projection $\textproc{pr}$ and an initial covariance $\mathbf{C}_\mathrm{init}\in\mathbb{R}^{N_\mathbf{q}\times N_\mathbf{q}}$. $\textproc{pr}$ is set to the identity function $\mathbf{lambda}\:\mathrm{mu}: \mathrm{mu}$, where $\mathbf{lambda}$ signals that this is a lambda function from Python, and $\mathbf{C}_\mathrm{init}$ is set to $\mathrm{None}$ if they are not specified.

$\textproc{pr}$ is used to project the inputs onto a given set and $\mathbf{C}_\mathrm{init}$ is an alternative definition for the initialization of the covariance. In this chapter we do not need to specify these inputs, however $\textproc{pr}$ could be used if we had an optimization problem where the iterates were restricted to a certain spatial domain, which is here not the case.

The implementation in pseudo code that is shown below corresponds to Algorithm $1$ in \cite{Keil2022-dj}. We have added some implementation details.

%\begin{algorithm}[H]%\footnotesize
%\caption{EnOpt algorithm}
%\begin{algorithmic}[1]
%\Function{EnOpt}{$F,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho,\operatorname{pr}=\operatorname{id}$}
%\State $F_{{0}}\gets F(\mathbf{q}_0)$
%\State $\mathbf{q}_{1},T_{1},\mathbf{C}_{\mathbf{q}_{0}}^{0},F_{1}\gets\mathrm{OptStep}(F,\mathbf{q}_0,N,0,\{\},0,F_{0},\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
%\State $k\gets 1$
%\While{$F_{k}>F_{k-1}+\varepsilon$\text{ and }$k<k^*$}
%\State $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_k,F_{k+1}\gets\mathrm{OptStep}(F,\mathbf{q}_k,N,k,T_k,\mathbf{C}_{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
%\State $k\gets k+1$
%\EndWhile
%\State \Return $\mathbf{q}^*\gets\mathbf{q}_k$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{\label{EnOptAlg}EnOpt algorithm}
\begin{algorithmic}[1]
\Function{EnOpt}{$\textproc{F},\mathbf{q}_0,N,\varepsilon,k^*,\beta_1,\beta_2,r,\nu^*,\sigma^2,\rho, N_t, N_b, \textproc{pr} \gets \mathbf{lambda}\:\mathrm{mu}: \mathrm{mu}, \mathbf{C}_\mathrm{init}\gets\mathrm{None}$}
\State \label{EnOptAlgFOMCall}$F^\mathrm{prev}_{k}\gets \Call{F}{\mathbf{q}_0}$
\State\label{EnOptAlgOptStepCall1} $\mathbf{q}_k,T_k,\mathbf{C}_k,F_k\gets\Call{OptStep}{\textproc{F},\mathbf{q}_0,N,0,[\;],\mathbf{C}_\mathrm{init},F^\mathrm{prev}_{k},\beta_1,\beta_2,r,\varepsilon,\nu^*,\sigma^2,\rho, N_t, N_b, \textproc{pr}}$
\State $k\gets 1$
\While{\label{FOMStoppingCriterionAlg}$F_{k}>F^\mathrm{prev}_k+\varepsilon$\text{ and }$k<k^*$}\label{EnOptAlgBeginWhile}
\State $F^\mathrm{prev}_k\gets F_k$
\State\label{EnOptAlgOptStepCall2} $\mathbf{q}_{k},T_{k},\mathbf{C}_k,F_{k}\gets\Call{OptStep}{\textproc{F},\mathbf{q}_k,N,k,T_k,\mathbf{C}_k,F_k,\beta_1,\beta_2,r,\varepsilon,\nu^*,\sigma^2,\rho, N_t, N_b,\textproc{pr}}$
\State $k\gets k+1$
\EndWhile\label{EnOptAlgEndWhile}
\State \Return $\mathbf{q}_k$
\EndFunction
\end{algorithmic}
\end{algorithm}

We begin by initializing $F^\mathrm{prev}_{k}$ as $\Call{F}{\mathbf{q}_0}$. The next iterate $\mathbf{q}_k$, along with the sample $T_k$, the covariance $\mathbf{C}_k$ and the functional value of $\mathbf{q}_k$, denoted by $F_k$, are computed by calling the initial optimization step $\textproc{OptStep}$, which is shown in Algorithm \ref{OptStep}.

After we initialized $k$ as $1$, we enter a while-loop until the stop criterion is satisfied. That is, we stop the loop when the functional value $F_k$ of the current iterate is no longer significantly larger than the functional value $F^\mathrm{prev}_k$ of the last iterate, so when $F_k\leq F^\mathrm{prev}_k+\varepsilon$. There is also a limit $k^*$ to the number of loop repetitions, so when that is reached, we also stop the while-loop. In each optimization loop, $F^\mathrm{prev}_{k}$ is set to the functional value of the last iterate and $\mathbf{q}_k,T_k,\mathbf{C}_k$ and $F_k$ are updated by calling $\textproc{OptStep}$ again. After we leave the while-loop, the last iterate $\mathbf{q}_k$ is returned.

The next algorithms use some functions from Python such as $\textproc{len}$ for the length of a list or an array, as well as functions from the Python package NumPy, which are identified by starting with '$\mathrm{np.}$'. The OptStep algorithm corresponds mostly to Algorithm $2$ in \cite{Keil2022-dj}. A change is that our algorithm can take an initial covariance matrix. Therefore the lines \ref{alg2differ1} to \ref{alg2differ2} differ with respect to the algorithm from \cite{Keil2022-dj}. Other differences are that the samples in line \ref{OptStepFOMCall1} are projected with $\textproc{pr}$ and the existence of the if-statement in line \ref{alg2differ3}.

%\begin{algorithm}[H]%\footnotesize
%\caption{OptStep algorithm}
%\begin{algorithmic}[1]
%\Function{OptStep}{$F,\mathbf{q}_k,N,k,T_k,\mathbf{C}_{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr}$}
%\If{$k=0$}
%\State \text{Compute the initial covariance matrix }$\mathbf{C}_0$\text{ like defined in }\eqref{initCov}\text{ with }$\mathbf{q}_0,\sigma^2,\rho$
%\Else
%\State $\mathbf{C}_k \gets \mathrm{updateCov}(\mathbf{q}_k, N, T_k, \mathbf{C}_{k-1}, F_k ,\tilde{\beta})$
%\EndIf
%\State \text{Sample }$N$\text{ control vectors }$\{\mathbf{q}_{k,j}\}_{j=1}^N$\text{ from a distribution }$\mathcal{N}(\mathbf{q}_{k},\mathbf{C}_k)$
%\State $T_{k+1}\gets\{\mathbf{q}_{k,j},F(\mathbf{q}_{k,j})\}_{j=1}^N$
%\State \text{Compute the vector }$\mathbf{C}_{\mathbf{q}_{k},F}^{k}$\text{ with }$\mathbf{q}_k,F_k$\text{ and the stored values of }$T_{k+1}\text{ like in }\eqref{CqkF}$
%\State \text{Compute the search direction }$\mathbf{d}_k=\mathbf{C}_{\mathbf{q}_{k},F}^{k}/\|\mathbf{C}_{\mathbf{q}_{k},F}^{k}\|_\infty$
%\State $\mathbf{q}_{k+1}\gets\mathrm{LineSearch}(F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr})$
%\State $F_{k+1}\gets F(\mathbf{q}_{k+1})$
%\State \Return $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_k,F_{k+1}$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{\label{OptStep}OptStep algorithm}
\begin{algorithmic}[1]
\Function{OptStep}{$\textproc{F},\mathbf{q}_k,N,k,T_k,\mathbf{C}_k,F_k,\beta_1,\beta_2,r,\varepsilon,\nu^*,\sigma^2,\rho, N_t, N_b, \textproc{pr} \gets \mathbf{lambda}\:\mathrm{mu}: \mathrm{mu}$}
\State $N_\mathbf{q}\gets\Call{len}{\mathbf{q}_k}$
\State $\mathbf{C}^\mathrm{next}_k \gets \mathrm{np.}\Call{zeros}{(N_\mathbf{q}, N_\mathbf{q})}$
\If{$k=0$}
\If{$\mathbf{C}_k$\text{ is }$\mathrm{None}$}\label{alg2differ1}
\State $\mathbf{C}^\mathrm{next}_k \gets \Call{initCov}{\protect\Call{len}{\mathbf{q}_k}, \sigma^2, \rho, N_t, N_b}$
\Else
\State $\mathbf{C}^\mathrm{next}_k \gets \mathbf{C}_k.\Call{copy}{\:}$
\EndIf\label{alg2differ2}
\Else
\State $\mathbf{C}^\mathrm{next}_k \gets \Call{updateCov}{\mathbf{q}_k, T_k, \mathbf{C}_k, F_k ,\beta_2}$
\EndIf
\State $\mathrm{sample} \gets \mathrm{np.random.}\Call{multivariate\_normal}{\mathbf{q}_k, \mathbf{C}^\mathrm{next}_k, \mathrm{size} \gets N}$
\State \label{OptStepFOMCall1}$T^\mathrm{next}_k\gets[\Call{pr}{\mathrm{sample}[j]},\Call{F}{\protect\Call{pr}{\mathrm{sample}[j]}}]_{j=0}^{N-1}$
\State $\mathbf{C}^k_F \gets \mathrm{np.}\Call{zeros}{N_\mathbf{q}}$
\For{$m=0,\dotsc,N-1$}
\State $\mathbf{C}^k_F \gets \mathbf{C}^k_F+(T^\mathrm{next}_k[m][0]-\mathbf{q}_k)\cdot(T^\mathrm{next}_k[m][1]-F_k)$
\EndFor
\State $\mathbf{C}^k_F \gets 1/(N-1)\cdot\mathbf{C}^k_F$
\State $\mathbf{d}_k \gets \mathrm{np.}\Call{zeros}{N_\mathbf{q}}$
\State $\mathbf{q}^\mathrm{next}_{k}, F^\mathrm{next}_k \gets \mathrm{np.}\Call{copy}{\mathbf{q}_k},F_k$
\If{{\bfseries not }$\mathrm{np.}\Call{all}{\mathbf{C}^k_F==0}$}\label{alg2differ3}
\State $\mathbf{d}_k \gets \mathbf{C}^k_F/\mathrm{np.}\Call{max}{\mathrm{np.}\protect\Call{abs}{\mathbf{C}^k_F}}$
\State \label{OptStepFOMCall2}$\mathbf{q}^\mathrm{next}_{k}, F^\mathrm{next}_k \gets \Call{LineSearch}{\textproc{F},\mathbf{q}_k,F_k,\mathbf{d}_k,\beta_1,r,\varepsilon,\nu^*,\textproc{pr}}$
\EndIf
\State \Return $\mathbf{q}^\mathrm{next}_k,T^\mathrm{next}_k,\mathbf{C}^\mathrm{next}_k,F^\mathrm{next}_k$
\EndFunction
\end{algorithmic}
\end{algorithm}

We start the $\textproc{OptStep}$ algorithm by updating the covariance matrix $\mathbf{C}_k$ or, if $k$ is zero, initializing it. In the case that $k$ is zero, we first check if there is a predefined covariance matrix $\mathbf{C}_k$. When there is one, $\mathbf{C}^\mathrm{next}_k$ is set to that matrix. Otherwise, we define $\mathbf{C}^\mathrm{next}_k$ by calling the function $\textproc{initCov}$, which sets the matrix like it is described in \eqref{initCov}.

If $k$ is not zero, we get the updated covariance matrix by calling $\textproc{updateCov}$, which is described in \eqref{updateCov} and Algorithm \ref{updateCovAlg}.

The covariance matrix $\mathbf{C}^\mathrm{next}_k$ is now used to get a Gaussian distributed sample with $N$ elements around $\mathbf{q}_k$. The projected samples $\Call{pr}{\mathrm{sample}[j]}$ and their respective functional values $\Call{F}{\protect\Call{pr}{\mathrm{sample}[j]}}$ for $j=0,\dotsc,N-1$ are stored in the list $T^\mathrm{next}_k$. The definition is here abbreviated as $[\Call{pr}{\mathrm{sample}[j]},\Call{F}{\protect\Call{pr}{\mathrm{sample}[j]}}]_{j=0}^{N-1}$. In the code, this is done with an iteration through a for-loop.

After that, we set $\mathbf{C}^k_F$ like it is defined in \eqref{CqkF}. Then we check if $\mathbf{C}^k_F$ is equal to the zero vector. That can happen if the functional F is constant on the sample set. In that case, we let the next iterate $\mathbf{q}^\mathrm{next}_k$ and the corresponding functional value $F_k$ unchanged.

Otherwise, we set $\mathbf{d}_k$ to $\mathbf{C}^k_F$ divided by its maximum norm $\mathrm{np.}\Call{max}{\mathrm{np.}\protect\Call{abs}{\mathbf{C}^k_F}}=\|\mathbf{C}^k_F\|_\infty$. The next iterate $\mathbf{q}^\mathrm{next}_{k}$ and its functional value $F^\mathrm{next}_k$ is now computed with the line search algorithm $\textproc{LineSearch}$, that is shown in Algorithm \ref{LineSearchAlg}.

We return $\mathbf{q}^\mathrm{next}_k,T^\mathrm{next}_k,\mathbf{C}^\mathrm{next}_k$ and $F^\mathrm{next}_k$.

%\begin{algorithm}[H]%\footnotesize
%\caption{Covariance matrix initialization}
%\begin{algorithmic}[1]
%\Function{initCov}{$\mathbf{q}_k, \sigma^2, \rho$}
%\State $N_\mathbf{q}\gets \mathrm{len}(\mathbf{q}_k)$
%\State \Return $$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm}[H]%\footnotesize
%\caption{Covariance matrix update}
%\begin{algorithmic}[1]
%\Function{updateCov}{$\mathbf{q}_k, N, T_k, \mathbf{C}_{k-1}, F_k ,\tilde{\beta}$}
%\State $N_\mathbf{q}\gets \mathrm{len}(\mathbf{q}_k)$
%\State $\mathbf{d}_k \gets \mathrm{np.zeros}((N_\mathbf{q}, N_\mathbf{q}))$
%\State $\mathrm{assert}$ $\mathrm{len}(T_k) == N$
%\For{$m=0,\dotsc,N-1$}
%\State $\mathbf{d}_k \gets d_k + (T_k[m][1]-F_k)*((T_k[m][0]-\mathbf{q}_k).\mathrm{reshape}((N_\mathbf{q},1))*(T_k[m][0]-\mathbf{q}_k).\mathrm{reshape}((1,N_\mathbf{q}))-\mathbf{C}_{k-1})$
%\EndFor
%\State $\mathbf{d}_k \gets \mathbf{d}_k/N$
%\State $\mathbf{C}_\mathrm{diag}\gets\mathrm{np.zeros}(N_\mathbf{q})$
%\State $\mathbf{d}_\mathrm{diag}\gets\mathrm{np.zeros}(N_\mathbf{q})$
%\For{$i=0,\dotsc,N_\mathbf{q}-1$}
%\State $\mathbf{C}_\mathrm{diag}[i]\gets\mathbf{C}_{k-1}[i, i]$
%\State $\mathbf{d}_\mathrm{diag}[i]\gets\mathbf{d}_k[i, i]$
%\EndFor
%\State $\tilde{\beta}_k\gets\tilde{\beta}$
%\While{$\mathrm{np.min}(\mathbf{C}_\mathrm{diag}+\tilde{\beta}_k*\mathbf{d}_k)<0$}
%\State $\tilde{\beta}_k \gets \tilde{\beta}_k/2$
%\EndWhile
%\State \Return $\mathbf{C}_{k-1}+\tilde{\beta}_k*\mathbf{d}_k$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{\label{updateCovAlg}Covariance matrix update}
\begin{algorithmic}[1]
\Function{updateCov}{$\mathbf{q}_k, T_k, \mathbf{C}_k, F_k ,\beta_2$}
\State $N_\mathbf{q}\gets \Call{len}{\mathbf{q}_k}$
\State $N\gets \Call{len}{T_k}$
\State $\mathbf{d}^\mathrm{cov}_k \gets \Call{np.zeros}{(N_\mathbf{q}, N_\mathbf{q})}$
%\State $\mathrm{assert}$ $\mathrm{len}(T_k) == N$
\For{$m=0,\dotsc,N-1$}
\State $\mathbf{d}^\mathrm{cov}_k \gets \mathbf{d}^\mathrm{cov}_k + (T_k[m][1]-F_k)\cdot((T_k[m][0]-\mathbf{q}_k).\Call{reshape}{(N_\mathbf{q},1)}\cdot(T_k[m][0]-\mathbf{q}_k).\Call{reshape}{(1,N_\mathbf{q})}-\mathbf{C}_k)$
\EndFor
\State $\mathbf{d}^\mathrm{cov}_k \gets \mathbf{d}^\mathrm{cov}_k/N$
\State \label{CDiagDDiagBegin} $\mathbf{C}_\mathrm{diag}, \mathbf{d}_\mathrm{diag}\gets\mathrm{np.}\Call{zeros}{N_\mathbf{q}}, \mathrm{np.}\Call{zeros}{N_\mathbf{q}}$
\For{$i=0,\dotsc,N_\mathbf{q}-1$}
\State $\mathbf{C}_\mathrm{diag}[i]\gets\mathbf{C}_k[i, i]$
\State \label{CDiagDDiagEnd}$\mathbf{d}_\mathrm{diag}[i]\gets\mathbf{d}^\mathrm{cov}_k[i, i]$
\EndFor
\State $\beta^\mathrm{iter}_2\gets\beta_2$
\While{$\mathrm{np.}\Call{min}{\mathbf{C}_\mathrm{diag}+\beta^\mathrm{iter}_2\cdot\mathbf{d}_\mathrm{diag}}\leq0$}
\State $\beta^\mathrm{iter}_2 \gets \beta^\mathrm{iter}_2/2$
\EndWhile
\State \Return $\mathbf{C}_k+\beta^\mathrm{iter}_2*\mathbf{d}^\mathrm{cov}_k$
\EndFunction
\end{algorithmic}
\end{algorithm}

For the update of the covariance matrix, we calculate the step direction $\mathbf{d}^\mathrm{cov}_k$ as in \ref{updateCovDirection} first. Then we want to make sure that the updated covariance matrix has only positive values on its diagonal.

For this purpose, $\mathbf{C}_\mathrm{diag}$ and $\mathbf{d}_\mathrm{diag}$ are defined from line \ref{CDiagDDiagBegin} to line \ref{CDiagDDiagEnd} as vectors whose elements are the diagonal values of $\mathbf{C}_k$ and $\mathbf{d}^\mathrm{cov}_k$ respectively. Then, the step size $\beta^\mathrm{iter}_2$, initialized as $\beta_2$, is halfed until $\mathrm{np.}\Call{min}{\mathbf{C}_\mathrm{diag}+\beta^\mathrm{iter}_2*\mathbf{d}_\mathrm{diag}}$ is positive. Finally, the matrix $\mathbf{C}_k+\beta^\mathrm{iter}_2*\mathbf{d}^\mathrm{cov}_k$, as in \eqref{updateCov}, is returned.

The next algorithm corresponds to Algorithm 3 in \cite{Keil2022-dj} with some added implementation details.

%\begin{algorithm}[H]%\footnotesize
%\caption{Line search}
%\begin{algorithmic}[1]
%\Function{LineSearch}{$F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr}$}
%\State $\beta_k \gets \beta$
%\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
%\State $\nu \gets 0$
%\While{$F(\mathbf{q}_{k+1})-F(\mathbf{q}_k)\leq\varepsilon$\text{ and }$\nu<\nu^*$}
%\State $\beta_k \gets r\beta_k$
%\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
%\State $\nu \gets \nu+1$
%\EndWhile
%\Return $\mathbf{q}_{k+1}$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{\label{LineSearchAlg}Line search}
\begin{algorithmic}[1]
\Function{LineSearch}{$\textproc{F}, \mathbf{q}_k, F_k, \mathbf{d}_k, \beta_1, r, \varepsilon, \nu^*, \textproc{pr}$}
\State $\beta^\mathrm{iter}_1 \gets \beta_1$
\State $\mathbf{q}^\mathrm{next}_k \gets \Call{pr}{\mathbf{q}_k+\beta^\mathrm{iter}_1\mathbf{d}_k}$
\State \label{LineSearchAlgFOMCall1}$F^\mathrm{next}_k \gets \Call{F}{\mathbf{q}^\mathrm{next}_k}$
\State $\nu \gets 0$
\While{$F^\mathrm{next}_k-F_k\leq\varepsilon$\text{ and }$\nu<\nu^*$}
\State $\beta^\mathrm{iter}_1 \gets r\beta^\mathrm{iter}_1$
\State $\mathbf{q}^\mathrm{next}_k \gets \Call{pr}{\mathbf{q}_k+\beta^\mathrm{iter}_1\mathbf{d}_k}$
\State \label{LineSearchAlgFOMCall2}$F^\mathrm{next}_k \gets \Call{F}{\mathbf{q}^\mathrm{next}_k}$
\State $\nu \gets \nu+1$
\EndWhile
\Return $\mathbf{q}^\mathrm{next}_k, F^\mathrm{next}_k$
\EndFunction
\end{algorithmic}
\end{algorithm}

At the start of the line search algorithm, we initialize the step size $\beta^\mathrm{iter}_1$ as $\beta_1$. Then we repeatedly calculate $\mathbf{q}^\mathrm{next}_k = \Call{pr}{\mathbf{q}_k+\beta^\mathrm{iter}_1\mathbf{d}_k}$ and $F^\mathrm{next}_k = \Call{F}{\mathbf{q}^\mathrm{next}_k}$ with simultaneous reduction of $\beta^\mathrm{iter}_1$ by multiplication with $r$ in a while-loop until either $F^\mathrm{next}_k-F_k>\varepsilon$ or $\nu\geq\nu^*$. After the termination of the while-loop, $\mathbf{q}^\mathrm{next}_k$ and $F^\mathrm{next}_k$ are returned.

The cause for stopping the while-loop also shows when $\mathbf{q}^\mathrm{next}_k$ is the last iterate of the EnOpt algorithm, as the EnOpt algorithm stops when the function value of the next iteration is not greater than the function value of the last iteration plus $\varepsilon$, i.e. when $F^\mathrm{next}_k\leq F_k+\varepsilon$. Hence, if the while-loop in Algorithm \ref{LineSearchAlg} terminates because $\nu\geq\nu^*$ while $F^\mathrm{next}_k-F_k\leq\varepsilon$ still holds, then we know already that the while-loop in Algorithm \ref{EnOptAlg} will also stop after this iteration.\\

Now we use the adaptive EnOpt algorithm to optimize our FOM objective functional $j$ from \eqref{FOMFunctionalEvaluationDef}. Since this is a maximization procedure and $j$ should be minimized, we apply $-j$, denoted by $-\textproc{j}$, to the EnOpt algorithm, which gives us Algorithm \ref{FOM-EnOpt}. We call this the FOM-EnOpt algorithm because the FOM functional $j$ is applied to the EnOpt algorithm \ref{EnOptAlg}. It is defined similar to Algorithm 4 in \cite{Keil2022-dj}.

%\begin{algorithm}[H]%\footnotesize
%\caption{FOM-EnOpt algorithm}
%\begin{algorithmic}[1]
%\Function{FOM-EnOpt}{$\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho$}
%\State \Return $\mathrm{EnOpt}(-j,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho)$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{\label{FOM-EnOpt}FOM-EnOpt algorithm}
\begin{algorithmic}[1]
\Function{FOM-EnOpt}{$\mathbf{q}_0,N,\varepsilon,k^*,\beta_1,\beta_2,r,\nu^*,\sigma^2,\rho, N_t, \mathbf{q}_\mathrm{base}$}
\State \Return $\Call{EnOpt}{-\textproc{j},\mathbf{q}_0,N,\varepsilon,k^*,\beta_1,\beta_2,r,\nu^*,\sigma^2,\rho, N_t, \protect\Call{len}{\mathbf{q}_\mathrm{base}}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

$\mathbf{q}_\mathrm{base}$ is here a list that consists of the shape functions, so $\Phi$ in \eqref{basisFuncionsList}. There are some more inputs that $\textproc{FOM-EnOpt}$ requires, but we omit these as they are only needed for the calculation of $\textproc{j}$.\\

Before we move on to the Adaptive-ML-EnOpt method, we want to discuss the computational effort of running the FOM-EnOpt algorithm. Most computations during the FOM-EnOpt procedure require only basic operations. The most expensive calculations are calls of the FOM functional $j$. In Algorithm \ref{EnOptAlg}, the only computation of the FOM objective functional value is in line \ref{EnOptAlgFOMCall}. The other objective functional values get returned by calls of the optimization step procedure which is Algorithm \ref{OptStep}. Here, the FOM functional value is calculated $N$ times in line \ref{OptStepFOMCall1} and also possibly multiple times in line \ref{OptStepFOMCall2} when the line search procedure, Algorithm \ref{LineSearchAlg}, is called. In Algorithm \ref{LineSearchAlg}, the functional value of $j$ is calculated once in line \ref{LineSearchAlgFOMCall1} and up to $\nu^*$ times in line \ref{LineSearchAlgFOMCall2}. Updating the covariance matrix in Algorithm \ref{updateCovAlg} does not require any FOM computations since all necessary objective functional values are already contained in $T_k$.

We note that calls of the OptStep algorithm on the FOM functional $j$ are expensive. In the next chapter, we introduce an algorithm that replaces most of these calls by computations of the EnOpt algorithm on a surrogate functional. This surrogate is a machine learning-based approximation of $j$ around the current iterate.