\chapter{ Ensemble‑based optimization algorithm}

%besser erklären warum man eine Kovarianzmatrix benötigt (natürlicher Gradient?)

The adaptive ensemble-based algorithm (EnOpt) is usually used to maximize the net present value of an oil recovery method with respect to a control vector. We begin by describing this algorithm for a general function $F:\mathbb{R}^{N_q}\to \mathbb{R}$ to iteratively solve the optimization problem
\begin{displaymath}
\operatorname*{maximize}_{\mathbf{q}\in\mathcal{D}}F(\mathbf{q}).
\end{displaymath}
We start with an initialization $\mathbf{q}_0$ and update it iteratively using a preconditioned gradient ascent method given by
\begin{displaymath}
\mathbf{q}_{k+1} = \mathbf{q}_k+\beta_k\mathbf{d}_k,
\end{displaymath}
\begin{displaymath}
\mathbf{d}_k\approx\frac{\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k}{\|\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k\|}_\infty,
\end{displaymath}
where $k=0,1,2,...$ denotes the optimization iteration. $\beta_k$ with $0<\beta_k\leq1$ is computed by using a line search. Furthermore, $\mathbf{C}_{\mathbf{q}_k}^k$ denotes the user-defined covariance matrix of the control variables at the $k$-th iteration and $\mathbf{G}_k$ is the approximate gradient of $F$ with respect to the control variables.

We define the initial covariance matrix $\mathbf{C}_{\mathbf{q}_0}^0$ so that the covariance between controls of different basis functions $\phi_i,\phi_j$ is zero and
\begin{displaymath}
\mathrm{Cov}(q_j^i,q_j^{i+h})=\sigma_j^2\rho^h\left(\frac{1}{1-\rho^2}\right),\text{ for all }h\in\{0,...,M-i\},
\end{displaymath}
where $\sigma_j^2>0$ is the variance for the basis function $\phi_j$ and $\rho\in(-1,1)$ the correlation coefficient.

That means that for $\mathbf{C}_j:=\left(\mathrm{Cov}(q_j^i,q_j^{k})\right)_{i,k}$ with $j=1,\dotsc,N_b$, we set
\begin{displaymath}
\mathbf{C}_{\mathbf{q}_0}^0 =
\begin{pmatrix}
  \mathbf{C}_1 & 0 & \cdots & 0 \\
  0 & \mathbf{C}_2 & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \mathbf{C}_{N_b} 
 \end{pmatrix}.
\end{displaymath}

To compute the step direction $\mathbf{d}_k$ at iteration step $k$, we sample $\mathbf{q}_{k,m}\in\mathcal{D}$ for $m=1,...,N$, with $N\in\mathbb{N}$, from a multivariate Gaussian distribution with mean $\mathbf{q}_k$ and covariance $\mathbf{C}_{\mathbf{q}_k}^k$ and define
\begin{displaymath}
\mathbf{C}_{\mathbf{q}_k,F}^k:=\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k)).
\end{displaymath}
Now, we set $\mathbf{d}_k=\frac{\mathbf{C}_{\mathbf{q}_k,F}^k}{\|\mathbf{C}_{\mathbf{q}_k,F}^k\|_\infty}$. This is valid since $\mathbf{C}_{\mathbf{q}_k,F}^k$ is an estimation of $\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k$, which can by shown like in \cite{OGUNTOLA2021109165}. Here, we begin with the Taylor expansion around $\mathbf{q}_k$ and get
\begin{eqnarray*}
&&F(\mathbf{q})=F(\mathbf{q}_k)+(\mathbf{q}-\mathbf{q}_k)^T\nabla F(\mathbf{q}_k)+O(\|\mathbf{q}-\mathbf{q}_k\|^2)\\
&\implies&F(\mathbf{q})-F(\mathbf{q}_k)=(\mathbf{q}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}-\mathbf{q}_k\|^2).
\end{eqnarray*}
Multiplying both sides by $(\mathbf{q}-\mathbf{q}_k)$ and setting $\mathbf{q}=\mathbf{q}_{k,m}$ yields
\begin{eqnarray*}
&&(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
&=&(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\mathbf{G}_k+O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3),
\end{eqnarray*}
where $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ are the remaining terms containing order $\geq3$ of $(\mathbf{q}_{k,m}-\mathbf{q}_k)$. Neglecting $O(\|\mathbf{q}_{k,m}-\mathbf{q}_k\|^3)$ gives by summation over all samples and multiplication of both sides with $\frac{1}{N-1}$:
\begin{eqnarray*}
&&\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))\\
&\approx&\left(\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T\right)\mathbf{G}_k\\
&\implies&\mathbf{C}_{\mathbf{q}_k,F}^k\approx\mathbf{C}_{\mathbf{q}_k}^k\mathbf{G}_k,
\end{eqnarray*}
since $\frac{1}{N-1}\sum_{m=1}^N(\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T$ is itself an approximation of $\mathbf{C}_{\mathbf{q}_k}^k$.\\

Using the samples $\{\mathbf{q}_{k,m}\}_{m=1}^N$ and the covariance matrix $\mathbf{C}_{\mathbf{q}_k}^k$ from the last iteration, we can update $\mathbf{C}_{\mathbf{q}_k}^k$ by setting
\begin{displaymath}
\mathbf{C}_{\mathbf{q}_{k+1}}^{k+1}=\mathbf{C}_{\mathbf{q}_k}^k+\tilde{\beta}_k\tilde{\mathbf{d}}_k\text{, with}
\end{displaymath}
\begin{displaymath}
\tilde{\mathbf{d}}_k=N^{-1}\sum_{m=1}^N(F(\mathbf{q}_{k,m})-F(\mathbf{q}_k))((\mathbf{q}_{k,m}-\mathbf{q}_k)(\mathbf{q}_{k,m}-\mathbf{q}_k)^T-\mathbf{C}_{\mathbf{q}_k}^k),
\end{displaymath}
where $\tilde{\beta}_k$ is a step size.

Now, we iterate until $F(\mathbf{q}_k)\leq F(\mathbf{q}_{k-1})+\varepsilon$, with $\varepsilon>0$. This gives us the following algorithms:
\begin{algorithm}[H]%\footnotesize
\caption{EnOpt algorithm}
\begin{algorithmic}[1]
\Procedure{EnOpt}{$F,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho,\operatorname{pr}=\operatorname{id}$}
\State $F_{{0}}\gets F(\mathbf{q}_0)$
\State $\mathbf{q}_{1},T_{1},\mathbf{C}_{\mathbf{q}_{0}}^{0},F_{1}\gets\mathrm{OptStep}(F,\mathbf{q}_0,N,0,\{\},0,F_{0},\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
\State $k\gets 1$
\While{$F_{k}>F_{k-1}+\varepsilon$\text{ and }$k<k^*$}
\State $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_{\mathbf{q}_{k}}^{k},F_{k+1}\gets\mathrm{OptStep}(F,\mathbf{q}_k,N,k,T_k,\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr})$
\State $k\gets k+1$
\EndWhile
\State \Return $\mathbf{q}^*\gets\mathbf{q}_k$
\EndProcedure
\end{algorithmic}
\end{algorithm}
The EnOpt algorithm takes the objective function $F:\mathbb{R}^{N_q}\to\mathbb{R}$, our initial iterate $\mathbf{q}_0\in\mathbb{R}^{N_q}$, the sample size $N\in\mathbb{N}$, the tolerance $\varepsilon>0$, the maximum number of iterations $k^*\in\mathbb{N}$, the initial step size $\beta>0$ for the compuation of the next iterate, the step size $\tilde{\beta}$ for the iteration of the covariance matrix, the step size contraction $r\in(0,1)$, the maximum number of step size trials $\nu^*\in\mathbb{N}$, the variance $\sigma^2\in\mathbb{R}^{N_b}$ with positive elements, the correlation coefficient $\rho\in(-1,1)$ and a projection $\operatorname{pr}$, where the default is the identity function $\operatorname{id}:x\to x$.
\begin{algorithm}[H]%\footnotesize
\caption{OptStep algorithm}
\begin{algorithmic}[1]
\Procedure{OptStep}{F,$\mathbf{q}_k,N,k,T_k,\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1},F_k,\beta,\tilde{\beta},r,\varepsilon,\nu^*,\sigma^2,\rho,\operatorname{pr}$}
\If{$k=0$}
\State \text{Compute the initial covariance matrix }$\mathbf{C}_{\mathbf{q}_0}^0$\text{ like defined with }$\mathbf{q}_0,\sigma^2,\rho$
\Else
\State \text{Compute the covariance matrix }$\mathbf{C}_{\mathbf{q}_{k}}^{k}\text{ like defined with }\mathbf{q}_k,F_k,T_k,\mathbf{C}_{\mathbf{q}_{k-1}}^{k-1},\tilde{\beta},N$
\EndIf
\State \text{Sample }$N$\text{ control vectors }$\{\mathbf{q}_{k,j}\}_{j=1}^N$\text{ from a distribution }$\mathcal{N}(\mathbf{q}_{k},\mathbf{C}_{\mathbf{q}_{k}}^{k})$
\State $T_{k+1}\gets\{\mathbf{q}_{k,j},F(\mathbf{q}_{k,j})\}_{j=1}^N$
\State \text{Compute the vector }$\mathbf{C}_{\mathbf{q}_{k},F}^{k}$\text{ with }$\mathbf{q}_k,F_k$\text{ and the stored values of }$T_{k+1}$
\State \text{Compute the search direction }$\mathbf{d}_k=\mathbf{C}_{\mathbf{q}_{k},F}^{k}/\|\mathbf{C}_{\mathbf{q}_{k},F}^{k}\|_\infty$
\State $\mathbf{q}_{k+1}\gets\mathrm{LineSearch}(F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr})$
\State $F_{k+1}\gets F(\mathbf{q}_{k+1})$
\State \Return $\mathbf{q}_{k+1},T_{k+1},\mathbf{C}_{\mathbf{q}_k}^k,F_{k+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]%\footnotesize
\caption{Line search}
\begin{algorithmic}[1]
\Procedure{LineSearch}{$F,\mathbf{q}_k,\mathbf{d}_k,\beta,r,\varepsilon,\nu^*,\operatorname{pr}$}
\State $\beta_k \gets \beta$
\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
\State $\nu \gets 0$
\While{$F(\mathbf{q}_{k+1})-F(\mathbf{q}_k)\leq\varepsilon$\text{ and }$\nu<\nu^*$}
\State $\beta_k \gets r\beta_k$
\State $\mathbf{q}_{k+1} \gets \operatorname{pr}(\mathbf{q}_k+\beta_k\mathbf{d}_k)$
\State $\nu \gets \nu+1$
\EndWhile
\Return $\mathbf{q}_{k+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
Now we use this algorithm to optimize our objective function $j$. Since this is a maximization procedure and $j$ should be minimized, we apply $-j$ to the EnOpt algorithm which gives us:
\begin{algorithm}[H]%\footnotesize
\caption{FOM-EnOpt algorithm}
\begin{algorithmic}[1]
\Procedure{FOM-EnOpt}{$\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho$}
\State \Return $\mathrm{EnOpt}(-j,\mathbf{q}_0,N,\varepsilon,k^*,\beta,\tilde{\beta},r,\nu^*,\sigma^2,\rho)$
\EndProcedure
\end{algorithmic}
\end{algorithm}