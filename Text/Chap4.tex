\chapter{Adaptive‑ML‑EnOpt algorithm}

In this chapter, we introduce the Adaptive-ML-EnOpt algorithm \cite{Keil2022-dj}, which is a modified version of the EnOpt algorithm. This algorithm is supposed to reduce the number of FOM evaluations by using a machine learning-based surrogate function, which improves the computation speed with respect to the EnOpt algorithm. Therefore, we introduce deep neural networks (DNNs) next. After that, the Adaptive-ML-EnOpt-algorithm is presented.

\section{Deep neural networks}

This description of deep neural networks is based on the definitions in \cite{Keil2022-dj}.

DNNs are used here to approximate a function $f:\mathbb{R}^{N_{\mathrm{in}}}\to\mathbb{R}^{N_{\mathrm{out}}}$ with $N_{\mathrm{in}},N_{\mathrm{out}}\in\mathbb{N}$. We call $L\in\mathbb{N}$ the number of layers and $N_{\mathrm{in}}=N_0,N_1,\dotsc,N_{L-1}, N_L=N_{\mathrm{out}}$ the number of neurons in each layer. $W_i\in\mathbb{R}^{N_i\times N_{i-1}}$ denotes the weights in layer $i\in\{1,\dotsc,L\}$ and $b_i\in\mathbb{R}^{N_i}$ the biases of the layer $i\in\{1,\dotsc,L\}$. These are composed as $\mathbf{W}=\left((W_1,b_1),\dotsc,(W_L,b_L)\right)$, which is a tuple of pairs of corresponding weights and biases.

$\rho:\mathbb{R}\to\mathbb{R}$ is the so-called activation function. A popular example is the rectified linear unit funtion $\rho(x)=\operatorname{max}(x,0)$, however we will use the hyperbolic tangent funtion:
\begin{displaymath}
\rho(x)=\tanh(x)=\frac{\exp(2x)-1}{\exp(2x)+1}.
\end{displaymath}
$\rho_n^*:\mathbb{R}^n\to\mathbb{R}^n$ is now defined as the component-wise application of $\rho$ onto a vector of dimension $n$, so $\rho_n^*(x)=\left[\rho(x_1),\dotsc,\rho(x_n)\right]^T$ for $x\in\mathbb{R}^n$.

To calculate the output $\Phi_\mathbf{W}(x)\in\mathbb{R}^{N_{\mathrm{out}}}$ of a DNN for an input $x\in\mathbb{R}^{N_{\mathrm{in}}}$, we apply the weights, biases, and activation function multiple times onto the input. It is calculated iteratively as shown here:
\begin{eqnarray*}
r_0(x)&:=&x,\\
r_i(x)&:=&\rho_{N_i}^*(W_ir_{i-1}(x)+b_i)\text{ for }i=1,\dotsc,L-1,\\
r_L(x)&:=&W_Lr_{L-1}(x)+b_L,\\
\Phi_\mathbf{W}(x)&:=&r_L(x).
\end{eqnarray*}

Now we try to optimize the parameters in $\mathbf{W}$ such that $\Phi_\mathbf{W}\approx f$. To achieve this, we sample a set that consists of inputs $x_i\in X\subset\mathbb{R}^{N_{\mathrm{in}}}$ and corresponding outputs $f(x_i)\in\mathbb{R}^{N_{\mathrm{out}}}$ and assemble them in the training set
\begin{equation}
T_\mathrm{train}=\{(x_1,f(x_1)),\dotsc,(x_{N_\mathrm{train}},f(x_{N_\mathrm{train}}))\}\subset X\times\mathbb{R}^{N_\mathrm{out}}.
\end{equation}
To evaluate the performance of our chosen $\mathbf{W}$, we use the mean squared error loss $\mathscr{L}(\Phi_\mathbf{W},T_\mathrm{train})$ to measure the distance between $\Phi_\mathbf{W}$ and $f$ on a training set. The mean squared error loss is defined as
\begin{displaymath}
\mathscr{L}(\Phi_\mathbf{W},T_\mathrm{train}):=\frac{1}{|T_\mathrm{train}|}\sum_{(x,y)\in T_\mathrm{train}}\| \Phi_\mathbf{W}(x)-y\|_2^2.
\end{displaymath}
Since we want $\Phi_\mathbf{W}$ to be close to $f$, we minimize the loss function with respect to $\mathbf{W}$. For that, we use some gradient-based optimization method. By the structure of the DNN, we can use the chain rule multiple times to divide the gradient of $\mathscr{L}$ into much simpler gradient computations.\\

We want that $\Phi_\mathbf{W}$ is close to $f$ on $X$ but we train it only on a sample set of $X$, so we achieve that $\Phi_\mathbf{W}$ is only on $T_\mathrm{train}$ close to $f$. While we train, the mean squared error loss will eventually get better and better on the training set, but at some point the error on different samples will get worse \cite{Prechelt2012}. We call that overfitting.

To prevent overfitting, we use early stopping. For early stopping, we evaluate the loss function on a validation set $T_\mathrm{val}\subset X\times\mathbb{R}^{N_\mathrm{out}}$, where usually $T_\mathrm{val}\cap T_\mathrm{train}=\emptyset$. Our algorithm for early stopping looks like this:

\begin{itemize}
\item let $\mathbf{W}^{(k)}$ be the weights in epoch $k$%evtl klarer machen, dass \mathbf{W}_k nicht W_k ist
\item compute $\mathscr{L}(\Phi_{\mathbf{W}^{(k)}},T_\mathrm{val})$ in each epoch
\item save $\mathbf{W}^{(k^*)}$ at iteration $k^*$ if it is the minimizer over all previous weights
\item if $\mathscr{L}(\Phi_{\mathbf{W}^{(k^*+i)}},T_\mathrm{val})\geq\mathscr{L}(\Phi_{\mathbf{W}^{(k^*)}},T_\mathrm{val})$ for all $i$ from $0$ to a prescribed number:\\
abort the training and use $\mathbf{W}^{(k^*)}$
\end{itemize}

So we abort the training if the minimum loss is not decreasing over a prescribed number of consecutive epochs. Our reasoning behind that is that the loss on the validation set is not srictly decreasing and can even increase over some epochs, but that is fine for us as long as we can decrease the loss over time.

To summarize, the training of one neural network is shown in algorithm \ref{trainDNN}. It takes the initialization of the neural network ($\mathrm{DNN}$), the inputs and outputs of the training set ($x_\mathrm{train}, y_\mathrm{train}$), the inputs and outputs of the testing set ($x_\mathrm{test}, y_\mathrm{test}$), the loss function ($\mathrm{loss\_fn}$), the optimizer ($\mathrm{optimizer}$), the number of training epochs ($\mathrm{epochs}$) and the number ($\mathrm{earlyStop}$) that describes after how many iterations without improvement of the test loss the training gets aborted. In our case, the loss function is chosen as the mean squared error loss and we use the L-BFGS optimizer with strong Wolfe line-search as our optimizer. The number of training epochs is only the maximum number of iterations since we apply early stopping to our training algorithm. Usually, the training terminates earlier because the loss over the testing set is not decreasing further.

The function `$\mathrm{testDNN}$' in algorithm \ref{trainDNN} returns the loss of the function $\mathrm{loss\_fn}$ between the output of the DNN with the current parameters and the output of the objective function over the testing set which are saved as $y_\mathrm{train}$. So if $\mathrm{loss\_fn}=\mathscr{L}$, we have $\mathrm{testDNN}(\mathrm{DNN}, x_\mathrm{test}, y_\mathrm{test}, \mathrm{loss\_fn}) = \mathscr{L}(\mathrm{DNN}, T_\mathrm{test})$ with
\begin{displaymath}
T_\mathrm{test}=\{((x_\mathrm{test})_1,(y_\mathrm{test})_1),\dotsc,((x_\mathrm{test})_{N_\mathrm{test}},(y_\mathrm{test})_{N_\mathrm{test}})\}.
\end{displaymath}

\begin{algorithm}[H]%\footnotesize
\caption{\label{trainDNN}DNN training}
\begin{algorithmic}[1]
\Function{trainDNN}{$\textproc{DNN}, x_\mathrm{train}, y_\mathrm{train}, x_\mathrm{test}, y_\mathrm{test}, \textproc{loss\_fn}, \textproc{optimizer}, \mathrm{epochs}, \mathrm{earlyStop}$}
\State $\mathrm{wait} \gets 0$
\State $\mathrm{minimalTestLoss} \gets \Call{testDNN}{\mathrm{DNN}, x_\mathrm{test}, y_\mathrm{test}, \mathrm{loss\_fn}}$
%\State \text{save the current parameters of the DNN}%$\mathrm{torch.save}(\mathrm{DNN.state\_dict}(), \mathrm{'checkpoint.pth'})$
\State $\Call{torch.save}{\textproc{DNN.state\_dict}(\:), \mathrm{'checkpoint.pth'}}$
\For{$\mathrm{epoch}=1,\dotsc,\mathrm{epochs}$}
%\State \text{do one training step with the }$\mathrm{optimizer}$
\State $\Call{DNN.train}{\:}$
\Function{closure}{\:}
    \State $\mathrm{y\_pred} \gets \Call{DNN}{\mathrm{x\_train}}\textproc{.reshape}(\Call{len}{\mathrm{y\_train}})$
    \State $\mathrm{loss} \gets \Call{loss\_fn}{\mathrm{y\_pred}, \mathrm{y\_train}}$
    \State $\Call{optimizer.zero\_grad}{\:}$
    \State $\mathrm{loss.}\Call{backward}{\:}$
    \State \Return $\mathrm{loss}$
\EndFunction
\State $\Call{optimizer.step}{\textproc{closure}}$
\State $\mathrm{testLoss} \gets \Call{testDNN}{\textproc{DNN}, x_\mathrm{test}, y_\mathrm{test}, \textproc{loss\_fn}}$
\If{$\mathrm{testLoss} < \mathrm{minimalTestLoss}$}
\State $\mathrm{wait} \gets 0$
\State $\mathrm{minimalTestLoss} \gets \mathrm{testLoss}$
%\State \text{save the current parameters of the DNN}%$\mathrm{torch.save}(\mathrm{DNN.state\_dict}(), \mathrm{'checkpoint.pth'})$
\State $\Call{torch.save}{\textproc{DNN.state\_dict}(\:), \mathrm{'checkpoint.pth'}}$
\Else
\State $\mathrm{wait} \gets \mathrm{wait}+1$
\EndIf
\If{$\mathrm{wait} \geq \mathrm{earlyStop}$}
%\State \text{overwrite the parameters of the DNN with the saved parameters}%$\mathrm{DNN.load\_state\_dict}(\mathrm{torch.load}(\mathrm{'checkpoint.pth'}))$
\State $\Call{DNN.load\_state\_dict}{\textproc{torch.load}(\mathrm{'checkpoint.pth'})}$
\State \Return
\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

Since we search for local minima of the loss function, the initial value $\mathbf{W}^{(0)}$ of our iteration effects the local optimum that we get and therefore the performance. We use Kaiming initialization \cite{7410480} to set our initial value $\mathbf{W}^{(0)}$. With Kaiming initialization, the starting values are initialized randomly since the elements of the weights $W_i$ are sampled from a zero-mean Gaussian distribution whose standard deviation is $\sqrt{2/N_{i-1}}$ for $i\in\{1,\dotsc,L\}$. The biases $b_i$ are set to zero for $i\in\{1,\dotsc,L\}$. The idea behind the random sampling is that the specified standard deviation prevents the exponential increase/ reduction of the input as shown in \cite{7410480}.

For the training of the DNN, we perform multiple restarts of the training algorithm with different initializations of $\mathbf{W}^{(0)}$ which minimizes the dependence of our neural network from the initial values. After we have trained enough DNNs, we select the neural network $\Phi_{\mathbf{W}^*}$ that has the smallest evaluation loss $\mathscr{L}(\Phi_{\mathbf{W}^*},T_\mathrm{val})$ over all restarts.

Before the whole algorithm for the construction of the DNN is presented, we look at the data that we use for the training. If we sample the inputs in a small area, it is likely that the corresponding outputs are also close to each other. Since we convert the values for the training of the neural network from double to float, it can even happen that the converted inputs or outputs are constant. In that case, the digits of these values that differ from each other get cut off at the conversion.

We want the values of the inputs and outputs to be distributed in such a way that significant differences are correctly represented. For that, the inputs $x\in\mathbb{R}^n$ and outputs $y\in\mathbb{R}$ are scaled to $\tilde{x}\in[0,1]^n$ and $\tilde{y}\in[0,1]$.

Let
\begin{equation}
T=\{(x_1,y_1),\dotsc,(x_N,y_N)\}
\end{equation}
be a sample set of size $N$ and
\begin{align*}
T_x&=\{x_1,\dotsc,x_N\},&T_y&=\{y_1,\dotsc,y_N\}
\end{align*}
the sets that contain the inputs/ output of that sample.

We define $x^\mathrm{low}, x^\mathrm{upp}\in\mathbb{R}^n$ and $y^\mathrm{low}, y^\mathrm{upp}\in\mathbb{R}$ as
\begin{eqnarray*}
x^\mathrm{low}_i&:=&\operatorname*{min}\{x_i\mid x\in T_x\}\text{ for }i=1,\dotsc,n,\\
x^\mathrm{upp}_i&:=&\operatorname*{max}\{x_i\mid x\in T_x\}\text{ for }i=1,\dotsc,n,\\
y^\mathrm{low}&:=&\operatorname*{min}T_y,\\
y^\mathrm{upp}&:=&\operatorname*{max}T_y.
\end{eqnarray*}

Now, $\tilde{x}$ and $\tilde{y}$ are calculated as
\begin{align*}
\tilde{x}_i&=\frac{x_i-x^\mathrm{low}_i}{x^\mathrm{upp}_i-x^\mathrm{low}_i}\text{ for }i=1,\dotsc,n,&\tilde{y}&=\frac{y-y^\mathrm{low}}{y^\mathrm{upp}-y^\mathrm{low}}.
\end{align*}

After we have trained the neural network, we need to rescale the DNN outputs so that we have a proper approximation of the function $f$. The output $\Phi(\tilde{x})$ of the DNN $\Phi$ is rescaled with the calculation $\Phi(\tilde{x})\cdot(y^\mathrm{upp}-y^\mathrm{low})+y^\mathrm{low}$.
%\begin{eqnarray*}
%\tilde{x}_i&=\frac{x_i-x^\mathrm{low}_i}{x^\mathrm{upp}_i-x^\mathrm{low}_i}&\text{ for }i=1,\dotsc,n,\\
%\tilde{y}&=\frac{y-y^\mathrm{low}}{y^\mathrm{upp}-y^\mathrm{low}},&
%\end{eqnarray*}

To summarize, we present now the construction of a DNN as pseudo code. Training parameters like the neural network structure, the activation function, the loss function, the optimizer and the number of training epochs are stored in $V_\mathrm{DNN}$. We denote $x^\mathrm{low}$ as $\mathrm{minIn}$, $x^\mathrm{upp}$ as $\mathrm{maxIn}$, $y^\mathrm{low}$ as $\mathrm{minOut}$ and $y^\mathrm{upp}$ as $\mathrm{maxOut}$. $\mathrm{minIn}$ and $\mathrm{maxIn}$ are calculated before the construction of the DNN and are taken as an input.

\begin{algorithm}[H]%\footnotesize
\caption{DNN construction}
\begin{algorithmic}[1]
\Function{constructDNN}{$\mathrm{sample}, V_\mathrm{DNN}, \mathrm{minIn}, \mathrm{maxIn}$}
%\State \text{scale inputs and outputs like described and save them as }$\mathrm{normSample}$\text{ and }$\mathrm{normVal}$
\State $\mathrm{normSample} \gets \Call{np.zeros}{\protect\Call{len}{\mathrm{sample}}, \protect\Call{len}{\mathrm{sample}[0][0]}}$
\State $\mathrm{normVal} \gets \Call{np.zeros}{\protect\Call{len}{\mathrm{sample}}}$
\For{$i = 0,\dotsc,\Call{len}{\mathrm{sample}}-1$}
\State $\mathrm{normSample}[i, :] \gets \mathrm{sample}[i][0]$
\State $\mathrm{normVal}[i] \gets \mathrm{sample}[i][1]$
\EndFor
\State $\mathrm{minOut} \gets \Call{np.min}{\mathrm{normVal}}$
\State $\mathrm{maxOut} \gets \Call{np.max}{\mathrm{normVal}}$
\State $\textproc{scaleInput} \gets \mathbf{lambda}\:\mathrm{mu}: (\mathrm{mu}-\mathrm{minIn})/(\mathrm{maxIn}-\mathrm{minIn})$
\State $\textproc{scaleOutput} \gets \mathbf{lambda}\:\mathrm{mu}: (\mathrm{mu}-\mathrm{minOut})/(\mathrm{maxOut}-\mathrm{minOut})$
\State $\textproc{rescaleOutput} \gets \mathbf{lambda}\:\mathrm{mu}: \mathrm{mu}\cdot(\mathrm{maxOut}-\mathrm{minOut})+\mathrm{minOut}$
\State $\mathrm{normSample} \gets \Call{scaleInput}{\mathrm{normSample}}$
\State $\mathrm{normVal} \gets \Call{scaleOutput}{\mathrm{normVal}}$
%\State \text{divide }$\mathrm{normSample}$\text{ and }$\mathrm{normVal}$\text{ into train/test splits }$x_\mathrm{train}, y_\mathrm{train}, x_\mathrm{test}, y_\mathrm{test}$
\State $x \gets \Call{torch.from\_numpy}{\mathrm{normSample}}\Call{.to}{\mathrm{torch.float32}}$
\State $y \gets \Call{torch.from\_numpy}{\mathrm{normVal}}\Call{.to}{\mathrm{torch.float32}}$
\State $\mathrm{train\_split} \gets \Call{int}{0.8 * \protect\Call{len}{x}}$
\State $x_\mathrm{train}, y_\mathrm{train} \gets x[:\mathrm{train\_split}], y[:\mathrm{train\_split}]$
\State $x_\mathrm{test}, y_\mathrm{test} \gets x[\mathrm{train\_split}:], y[\mathrm{train\_split}:]$
\State $\textproc{DNN} \gets \Call{FullyConnectedNN}{V_\mathrm{DNN}[0], \textproc{activation\_function}=V_\mathrm{DNN}[1]}$
\State $\textproc{loss\_fn} \gets \Call{nn.MSELoss}{\:}$
\State $\textproc{optimizer} \gets \Call{torch.optim.LBFGS}{\protect\Call{DNN.parameters}{\:}, \mathrm{lr}\gets\mathrm{learning\_rate}, \mathrm{line\_search\_fn}\gets\mathrm{'strong\_wolfe'}}$
\State $\Call{trainDNN}{\textproc{DNN}, x_\mathrm{train}, y_\mathrm{train}, x_\mathrm{test}, y_\mathrm{test}, \textproc{loss\_fn}, \textproc{optimizer}, \mathrm{epochs}}$
\State $\textproc{evalDNN} \gets \Call{testDNN}{\textproc{DNN}, x_\mathrm{test}, y_\mathrm{test}, \mathrm{loss\_fn}}$
\For{$i=1,\dotsc,\mathrm{numberOfRestarts}$}
\State $\textproc{DNN}_i \gets \Call{FullyConnectedNN}{V_\mathrm{DNN}[0], \textproc{activation\_function}=V_\mathrm{DNN}[1]}$
\State $\textproc{optimizer} \gets \Call{torch.optim.LBFGS}{\protect\Call{DNN$_i$.parameters}{\:}, \mathrm{lr}\gets\mathrm{learning\_rate}, \mathrm{line\_search\_fn}\gets\mathrm{'strong\_wolfe'}}$
\State $\Call{trainDNN}{\mathrm{DNN}_i, x_\mathrm{train}, y_\mathrm{train}, x_\mathrm{test}, y_\mathrm{test}, \textproc{loss\_fn}, \textproc{optimizer}, \mathrm{epochs}}$
\State $\textproc{evalDNN}_i \gets \Call{testDNN}{\textproc{DNN}_i, x_\mathrm{test}, y_\mathrm{test}, \textproc{loss\_fn}}$
\If{$\mathrm{evalDNN}_i<\mathrm{evalDNN}$}
\State $\mathrm{evalDNN} \gets \mathrm{evalDNN}_i$
\State $\mathrm{DNN} \gets \mathrm{DNN}_i$
\EndIf
\EndFor
\State $F_\mathrm{ML} \gets \mathbf{lambda}\:\mathrm{mu}: \Call{rescaleOutput}{\protect\Call{DNN}{\protect\Call{scaleInput}{\mathrm{mu}}}}$
\State \Return $F_\mathrm{ML}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Modifying the EnOpt algorithm by using a neural network-based surrogate}
 For the next step, we use neural networks like in \cite{Keil2022-dj} to get an improved version of the EnOpt algorithm. Here, we want to replace calls of the FOM function by a surrogate that is a neural network. The surrogate is supposed to be a local approximation of the FOM function around the current iterate. Doing that globally with a sufficient precision would be computationally too expensive.
 
 Since the surrogate is only locally exact, we will introduce a trust region method. For that, we use an algorithm that projects inputs into the trust region.
 
 \begin{algorithm}[H]%\footnotesize
\caption{Projection}
\begin{algorithmic}[1]
\Function{TR-Projection}{$x, \mathbf{q}_k, \mathbf{d}_k$}
\State $\mathrm{upp}\gets\mathbf{q}_k+\mathbf{d}_k$
\State $\mathrm{low}\gets\mathbf{q}_k-\mathbf{d}_k$
\State \Return $\Call{np.maximum}{\protect\Call{np.minimum}{x,\mathrm{upp}},\mathrm{low}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

We describe now the Adaptive-ML-EnOpt algorithm. This algorithm takes the initial guess $\mathbf{q}_0\in\mathbb{R}^{N_u}$, the sample size $N\in\mathbb{N}$, the tolerances $\varepsilon_o,\varepsilon_i>0$ for the outer and inner iterations, the maximum number of outer and inner iterations $k_o^*,k_i^*\in\mathbb{N}$, the DNN-specific variables $V_{\mathrm{DNN}}$, the initial step size $\beta>0$, the step size contraction $r\in(0,1)$ and the maximum number of step size trials $\nu^*\in\mathbb{N}$.
\begin{algorithm}[H]%\footnotesize
\caption{\label{AML-EnOpt}Adaptive-ML-EnOpt algorithm}
\begin{algorithmic}[1]
\Function{AML-EnOpt}{$F,\mathbf{q}_0,N,\varepsilon_o,\varepsilon_i,k_o^*,k_i^*,V_{\mathrm{DNN}},\beta_1, \beta_2,r,\nu^*,\sigma^2,\rho$}
\State $F_k\gets F(\mathbf{q}_0)$
\State $F^\mathrm{next}_k \gets F_k$
\State $\tilde{\mathbf{q}}_k,T_k,\mathbf{C}_k,\tilde{F}_k\gets\Call{OptStep}{F, \mathbf{q}_0, N, 0, [], \mathrm{None}, F_k, \beta_1, \beta_2, r, \varepsilon_o, \nu^*, \sigma^2, \rho}$
\State $k\gets 1$
\State $\mathbf{q}_k \gets \mathbf{q}_0$
\State $\mathbf{q}^\mathrm{next}_k \gets \mathbf{q}_k.\Call{copy}{\:}$
\State $\delta \gets 40$
\While{$\tilde{F}_k>F_k+\varepsilon_o$\text{ and }$k<k_o^*$}
%\State $\text{compute }\mathrm{minIn}\text{ and }\mathrm{maxIn}$
\State $T^x_k \gets \Call{np.zeros}{(N, nt+1)}$
\For{$i=0,\dotsc,N-1$}
\State $T^x_k[i,:] \gets T_k[i][0]$
\EndFor
\State $\mathrm{minIn} \gets \Call{np.zeros}{nt+1}$
\State $\mathrm{maxIn} \gets \Call{np.zeros}{nt+1}$
\For{$i=0,\dotsc,nt-1$}
\State $\mathrm{minIn}[i] \gets \Call{np.min}{T^x_k[:,i]}$
\State $\mathrm{maxIn}[i] \gets \Call{np.max}{T^x_k[:,i]}$
\EndFor
%\State $\mathbf{d}_k \gets \Call{np.abs}{\mathbf{q}_k-\tilde{\mathbf{q}}_k}$
%\State $F_\mathrm{ML}^k\gets\mathrm{Train}(T_k, V_\mathrm{DNN}, \mathrm{minIn}, \mathrm{maxIn})$
\State $\mathbf{d}_k\gets|\mathbf{q}_k-\tilde{\mathbf{q}}_k|$
\While{$F^\mathrm{next}_k\leq F_k+\varepsilon_o$}
\State $F_\mathrm{ML}^k\gets \Call{constructDNN}{T_k, V_\mathrm{DNN}, \mathrm{minIn}, \mathrm{maxIn}}$
\State $F^\mathrm{approx}_k\gets F_\mathrm{ML}^k(\mathbf{q}_k)$
\State $\mathrm{flag}_\mathrm{TR}\gets \mathrm{True}$
\While{$\mathrm{flag}_\mathrm{TR}$}
\State $\mathbf{d}^\mathrm{iter}_k\gets\delta\cdot\mathbf{d}_k$
\State $\mathbf{q}^\mathrm{next}_k\gets\Call{EnOpt}{F_\mathrm{ML}^k,\mathbf{q}_k,N,\varepsilon_i,k_i^*,\beta_1, \beta_2,r,\nu^*, \sigma^2, \rho, \mathbf{lambda}\:\mathrm{mu}:\protect\Call{TR-Projection}{\mathrm{mu}, \mathbf{q}_k, \mathbf{d}^\mathrm{iter}_k}, \mathbf{C}_\mathrm{init}\gets\mathbf{C}_k}[0]$
\State $F^\mathrm{next}_k\gets F(\mathbf{q}^\mathrm{next}_k)$
\State $\rho_k\gets \frac{F^\mathrm{next}_k-F_k}{\Call{F$_\mathrm{ML}^k$}{\mathbf{q}^\mathrm{next}_k}-F^\mathrm{approx}_k}$
\If{$\rho_k<0.25$}
\State $\delta\gets0.25\cdot\delta$
\EndIf
\If{$\rho_k>0.75$\text{ and }$\Call{np.any}{\protect\Call{np.abs}{\mathbf{q}_k-\mathbf{q}^\mathrm{next}_k}-\mathbf{d}^\mathrm{iter}_k == 0}$}
\State $\delta\gets2\cdot\delta$
\EndIf
\If{$\rho_k>0$}
\State $\mathrm{flag}_\mathrm{TR}\gets\mathbf{False}$
\EndIf
\EndWhile
\EndWhile
\State $\tilde{\mathbf{q}}_k,T_k,\mathbf{C}_k,\tilde{F}_k\gets\mathrm{OptStep}(F,\mathbf{q}^\mathrm{next}_k,N,k,T_k,\mathbf{C}_k, F^\mathrm{next}_k, \beta_1, \beta_2, r, \varepsilon ,\nu^*, \sigma^2, \rho)$
\State $F_k \gets F^\mathrm{next}_k$
\State $\mathbf{q}_k \gets \mathbf{q}^\mathrm{next}_k\Call{.copy}{\:}$
\State $k\gets k+1$
\EndWhile
\State \Return $\mathbf{q}^*\gets\mathbf{q}_k$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]%\footnotesize
\caption{\label{ROM-EnOpt}ROM-EnOpt algorithm}
\begin{algorithmic}[1]
\Function{ROM-EnOpt}{$\mathbf{q}_0,N,\varepsilon_o,\varepsilon_i,k_o^*,k_i^*,V_{\mathrm{DNN}},\beta_1, \beta_2,r,\nu^*,\sigma^2,\rho$}
\State \Return \Call{AML-EnOpt}{$-j,\mathbf{q}_0,N,\varepsilon_o,\varepsilon_i,k_o^*,k_i^*,V_{\mathrm{DNN}},\beta_1, \beta_2,r,\nu^*,\sigma^2,\rho$}
\EndFunction
\end{algorithmic}
\end{algorithm}